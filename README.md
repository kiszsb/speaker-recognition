# Speaker recognition 

Keyword-based speaker identification. A person speaks the keyword. Based on the
recording, the system should tell whether this person is known (authorized) and return his or
her ID.

## Data
It was decided to use the existing dataset consisting voice samples of 25 speakers, with 10
recordings of each speaker saying the phrase “nazywam sie”. Files are saved in .wav
format. Data was divided into 25 folders dedicated for each speaker.
5 samples from each folder were used in the training set, the other 5 were used in the testing
set.

## Detailed algorithm description 
Speaker’s process verification consists of following steps:
- Adding speaker to the database - extraction of distinct voice features for each
speaker
- Verification - comparing speaker’s voice with trained model based on distinct features
of speakers existing in the database


The feature extraction process consists of converting the speech signal into a set of acoustic
features that will allow the speaker to be identified.Selected audio feature extraction method
is the Mel-frequency cepstral coefficients (MFCC) which have 39 features.
It was decided to use parametric algorithms, which operate on the basis of known or
estimated information based on the training sequence.
- GMM (Gaussian mixture models) - the speaker model produces a single multivariate
normal distribution
- SVM (Support Vector Machines) - binary classifiers that make a decision based on a
pre-constructed class boundary. A decision rule must be found that, based on certain
observations, assigns a new, previously unknown object to one of many classes.

### GMM (Gaussian mixture models)
GMM is a type of clustering algorithm. Each custer is modeled according to a different
Gaussian distribution. This means that we don’t have hard assignments into clusters (like
k-means - data point is either in a cluster or not), but soft ones. Each data point could have
been generated by any of the distributions with a corresponding probability. We get the
probability of each point in each cluster and choose the one with the highest value.

Modeling was done using 5 voice samples per speaker creating a gmm model for each of
them. To verify proper speaker modeling, a test set was used, which consisted of another 5 voice
samples per speaker. 3 speakers weren’t included in modeling to check what happens when the speaker isn’t
present in the database. Accuracy of the model on this dataset for 16 mixtures is 98,2%. For some reason speaker 3
is classified as speaker 12 two times.
Speaker 3 and speaker 12 are both male with deep voices. They are quite similar, but
human wouldn’t confuse them. Perhaps wrong classification is caused by not great quality of
recording provided by speaker 12. Voice samples of both speakers were played for 5 people
and each of them decided that voices are not the same.
Model was also trained for 10, 5 and 2 mixtures. For 10 and 5 mixtures accuracy of the
model was 100%. For 2 mixtures accuracy was 99%.That could mean that the model using 16 mixtures was overtrained and it would be better to
use an algorithm with lesser mixtures.

### SVM (Support Vector Machines)
The Support vector machine classifier works by finding the hyperplane that maximizes the
margin between the two classes.

Modeling was done using 5 voice samples per speaker creating a svm model for each of
them. Next we are using SVC (Support Vector Classifier),which is an algorithm that can be used to
analyze and classify data. Accuracy of the model on this dataset is 100%.
Algorithm was tested with different amounts of MFCC features -> for 10 it is still 100% of
accuracy, but for 5 it is 96,36%. Even with just 3 features, accuracy is still quite high 72,72%
and for the most extreme version with only 1 MFCC feature accuracy is 20%.
Tests below were made with 5 MFCC features.
We were using implemented in sklearn Support Vector Classification. We tested different
values for class_weight (for ‘balanced’ we got 96,36 % accuracy, for none we got only 80%).
Which is understandable because if class_weight is not given, all classes are supposed to
have one weight. The “balanced” mode uses the values of y to automatically adjust weights
inversely proportional to class frequencies in the input data as n_samples / (n_classes *
np.bincount(y)).
We could also manipulate the parameter C which is the regularization parameter. The
strength of the regularization is inversely proportional to C. The best accuracy we got for
C=10, accuracy 98,18%, for C=5, accuracy 97,27% and for C=1 accuracy 96,36%.
